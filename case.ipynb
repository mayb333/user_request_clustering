{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0bc744-82d1-4a38-a435-a012c3b640dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas pyarrow numpy matplotlib seaborn top2vec IProgress sentence-transformers bertopic nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12f369a-af5c-4889-91c0-7dd73b4fbdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: six>=1.10.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: tensorboard<1.9.0,>=1.8.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.62.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.42.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in ./venv/lib/python3.10/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: html5lib==0.9999999 in ./venv/lib/python3.10/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (0.9999999)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: bleach==1.5.0 in ./venv/lib/python3.10/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=0.11.10->tensorboard<1.9.0,>=1.8.0->tensorflow) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a1ef18c-e900-495e-b1fd-88378c2c7efd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (pywrap_tensorflow_internal.py, line 114)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[13], line 6\u001b[0m\n    from umap import UMAP\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/umap/__init__.py:7\u001b[0m\n    from .parametric_umap import ParametricUMAP\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/umap/parametric_umap.py:14\u001b[0m\n    import tensorflow as tf\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/tensorflow/__init__.py:24\u001b[0m\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py:49\u001b[0m\n    from tensorflow.python import pywrap_tensorflow\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:58\u001b[0;36m\n\u001b[0;31m    from tensorflow.python.pywrap_tensorflow_internal import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/intercom/venv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:114\u001b[0;36m\u001b[0m\n\u001b[0;31m    def TFE_ContextOptionsSetAsync(arg1, async):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "import ssl\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468d9ae-2fb5-4d8b-8887-4c7cfe195657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TEST.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df[\"timestamp\"] = pd.to_datetime(df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29efb1-003c-42d4-847e-68d10c6cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Размер данных: {df.shape[0]}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c4b56-2a7e-467e-919c-43e772ff5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d644075-cbfb-4b3e-9c83-f951ec952ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Количество уникальных сессий: {df.session_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28aa19-84e6-4ec4-ac60-c094bf5f3841",
   "metadata": {},
   "source": [
    "Посмотрим пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91b923-fdfb-4161-b9ee-c9af12139a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2907a-77dd-4266-99ab-4352f4f8cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.session_id.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ee064-9c82-4da6-a592-a09c0b255f7d",
   "metadata": {},
   "source": [
    "Есть один пропуск, удалим его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5e7c5-f059-42dd-9d90-e7637a280504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.session_id.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37593a9a-78a4-4131-a7c8-1eb4eb09b0d0",
   "metadata": {},
   "source": [
    "Отсортируем данные по `timestamp` и выделим оттуда фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87c154-72b5-4742-b696-933d56420ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad289b-a1d0-4ab5-bd75-4e5fc59b64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df.timestamp.dt.date\n",
    "df[\"day_of_week\"] = df.timestamp.dt.day_of_week\n",
    "df[\"hour\"] = df.timestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632a99-e207-4d6c-835d-8bc151a5d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b39f5-fe0c-4a02-99ed-0e844fbaad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c9193-95aa-4f9f-b42f-aa0753fa6b42",
   "metadata": {},
   "source": [
    "Так, получается, что предоставленные данные урезаны до двух дней, тем не менее можем посмотреть распределение обращений по часам.\n",
    "\n",
    "Нельзя сказать, что это будет репрезентативно, да еще и данные предоставлены сразу после Нового года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd2dc9-61bb-49f9-82eb-97bb6cf259d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df[df.role == \"user\"].groupby(\"hour\").session_id.nunique())\n",
    "\n",
    "plt.title(\"Распределение сессий по часам\")\n",
    "plt.ylabel(\"quantity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca889c-4d06-4d3a-9717-42a0fd7a9271",
   "metadata": {},
   "source": [
    "Можем заметить, что к чат-боту чаще всего обращаются в 9 часов, что примерно в начале рабочего дня. Возможно какие-то проблемы с запуском оборудования.\n",
    "\n",
    "Но тем не менее обращения в течение дня так же есть. Скорее всего они связаны с какими-то поломками во время работы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba1cc1-40b7-4b8d-8d8a-bfa7f544e92c",
   "metadata": {},
   "source": [
    "# **Кластеризация первых сообщений пользователей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ff1bc-5acf-46f8-ad9a-50faf271f237",
   "metadata": {},
   "source": [
    "Посмотрим пример одной сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb98ba-ef7b-4837-9345-010a97eee253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"session_id\").text.apply(lambda x: list(x)).iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e787c9d-1ebb-4fbc-aa99-e5f579fd5c96",
   "metadata": {},
   "source": [
    "**Нам нужно кластеризировать обращения пользователей.**\n",
    "\n",
    "Здесь важный момент: мы хотим понять с какими запросами к нам приходят пользователи и провести какую-то аналитику или же, что более веротяно в контексте чат-бота, *мы хотим понимать к какому кластеру относится новый запрос пользователя и использовать это при построении ответа.*\n",
    "\n",
    "Если говорить о первой задаче, то тут уместнее использовать весь контекст сессии, чтобы лучше выделить ключевые моменты и сделать хороший анализ.\n",
    "\n",
    "**В продакшене же мы не можем знать заранее всю нашу сессию, поэтому нам нужно уметь хорошо кластерезировать по первому сообщения пользователя.**\n",
    "\n",
    "Возможно нужно брать первые 2 сообщения пользователя, так как иногда первое сообщение носит характер приветственого, а второе уже для раскрытия проблемы, но не везде. \n",
    "\n",
    "Например здесь вообще нет второго сообщения от пользователя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3638a1-a386-4360-b121-683d2279760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"session_id\").text.apply(lambda x: list(x)).iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcb44c-5144-4015-b07e-307c4f78b5ed",
   "metadata": {},
   "source": [
    "**Пока что будет использовать только самое первое сообщение в сессии от пользователя**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7c5e5-f806-4e54-a382-7943a007e9c0",
   "metadata": {},
   "source": [
    "Достанем первой сообщение из сессии от пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0a289-2e0f-4b2f-b430-8b427495cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[df.role == \"user\"].groupby(\"session_id\").text.apply(lambda x: list(x)[0]).tolist()\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f9ca9-cfb7-4bd4-aee2-c23b8af80f22",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df226f-adb9-46d1-b742-3ef8927c72d8",
   "metadata": {},
   "source": [
    "**Пайплайн кластеризации:**\n",
    "\n",
    "**Получение эмбеддингов обращений -> сокращение размерности -> кластеризация точек -> понимание что отличает кластеры друг от друга (BOW + c-TF-IDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e742b-e75f-4920-9a64-f8355355853d",
   "metadata": {},
   "source": [
    "Будем использовать **BERTopic.** \n",
    "\n",
    "Параметры подберем позже"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2c07a-dec8-4f49-bf4d-5b7c09273a39",
   "metadata": {},
   "source": [
    "Вычислим заранее эмбеддинги для постов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d62701-ae49-4d04-acd3-c3b268cee001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "embeddings = embedding_model.encode(docs)\n",
    "\n",
    "embeddings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7e538-6bb5-42b4-a397-800275e4bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=1)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer()\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31c33d-d8b0-46c4-a85f-a4182dfbed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(documents=processed_docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02720ff-f6b0-4fe2-8731-aa35cb99e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc95f8-4dae-4a14-8ca5-5e3ba19aab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.str.contains(\"jpeg\")].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de124fd7-ad0f-47e2-93c0-79c0f11a4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3507de-6b03-4d50-aee0-2174b2487a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "topic_model.visualize_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faddfcd8-5995-46f9-9ccc-907353359d89",
   "metadata": {},
   "source": [
    "Посмотрим на визуальное представления наших кластеров и сделаем парочку выводов:\n",
    "\n",
    "* Некоторые пользователи не сразу пишут свою проблему: в первом сообщении они сначала пишут \"Добрый день\" или \"Здравствуйте\", а уже потом вероятно обозначают свой запрос **(Кластеры 2, 5)**. Некоторые же присылают первое сообщение \"Оператор\", видимо чтобы проверить, что бот работает **(Кластер 3)**\n",
    "* Некоторые пользователи в первом же сообщении сессии уже выражают благодарность или присылают какой-то файл. Гипотеза в том, что вероятно у них есть доступ к предыдущей сессии, которая была завершена из-за молчания пользователя, но после пользователь все же разрешил эту проблему и отправил это сообщение, но уже в новую сессию **(Кластер 1, 6)**\n",
    "* У других пользователей проблема с сим-картой **(Кластер 4)**\n",
    "* Ну и основной кластер с наибольшим количеством объектов **(Кластер 0)**. В нем неудачно выделились ключевые слова, и мы не особо понимаем проблемы пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cecc7-4cdc-420b-88aa-1caa771bf0ac",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e2f17-be7e-43de-808f-ea273bb1facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = topic_model.get_document_info(docs)\n",
    "\n",
    "print(f\"Размер: {df_topics.shape[0]}\")\n",
    "df_topics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0d7fb-235f-467b-98a2-f4a003a7d9d8",
   "metadata": {},
   "source": [
    "**Для простоты оставим только Кластеры -1, 0, 4 и проанализируем их.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5f9a2-de66-4b46-859e-3e1179c4b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = df_topics[(df_topics[\"Topic\"] == 0) | (df_topics[\"Topic\"] == 4) | (df_topics[\"Topic\"] == -1)][\"Document\"].tolist()\n",
    "new_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cccffb-3c3a-4d65-a853-4e2c1bbe7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emb = embedding_model.encode(new_docs)\n",
    "new_emb[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9cbc6-cb74-443a-8da0-795a6ed81603",
   "metadata": {},
   "source": [
    "Предобработаем текст, который будем использовать для вычленения ключевых слов, характерезующих кластер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a849851-5c01-4851-b19f-169b2ccba874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "noise_list = [\"привет\", \"доброе утро\", \"добрый день\", \"добрый вечер\", \"здравствуйте\", \"до свидания\", \n",
    "              \"спасибо\", \"хорошо\", \"пожалуйста\", \"c новым годом\"]\n",
    "\n",
    "def remove_noise(text: str) -> str:\n",
    "    for noise in noise_list:\n",
    "        text = text.replace(noise, \"\")\n",
    "    return text\n",
    "\n",
    "def process_text(text: str, lemmatizer: WordNetLemmatizer = lemmatizer) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = remove_noise(text)\n",
    "    \n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in russian_stopwords]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513a89c-0548-4f03-95b0-4eaa732060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_new_docs = [process_text(doc) for doc in new_docs]\n",
    "processed_new_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d1707-cd33-4b74-ad91-8a3651d840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=15, min_dist=0.0, metric='cosine', random_state=1)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='leaf', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010d024-aa22-4b3a-993f-da33c0775d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(documents=processed_new_docs, embeddings=new_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ebbf0-a707-490d-8549-d15092b19924",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43aaef-30b2-47db-a308-d0849822e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "topic_model.visualize_documents(new_docs, topics=[-1, 0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af9b04-4242-4f27-a738-af3fdfc4b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics(topics=[-1, 0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c96637-cf2e-4390-9d60-fc56ad27c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8d5c1-f64b-4473-ad5a-92401897a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94146ba-bb67-437c-bffb-aad458342a86",
   "metadata": {},
   "source": [
    "Исходя из представленных ключевых слов и репрезентативные документы, кажется, что кластеры сформированы довольно логично и адекватно отражают различные тематические области:\n",
    "\n",
    "* **Кластер -1** `Ошибки, Проблемы с ОФД`\n",
    "    - Этот кластер кажется сосредоточенным вокруг технических ошибок, связанных с облачным фискальным документооборотом (ОФД) и не только.\n",
    "* **Кластер 0:** `Технические и операционные вопросы работы терминалов и касс`\n",
    "    - Включает вопросы функционирования терминалов, проблемы при проведении безналичных оплат и вопросы работы кассовых аппаратов, включая комплексные системы, такие как Эвотор.\n",
    "* **Кластер 1:** `Вопросы по оплате приложений`\n",
    "    - Фокусируется на проблемах и запросах, связанных с процедурой оплаты в различных приложениях.\n",
    "* **Кластер 2:** `Оплата сим-карт через терминалы и приложения`\n",
    "    - Основная тематика вопросов касается процесса оплаты сим-карт, включая трудности при оплате через терминалы и мобильные приложения.\n",
    "* **Кластер 3:** `Вопросы налогообложения, регистрации касс и системы патентов`\n",
    "    - Включает сложные вопросы, связанные с изменением систем налогообложения, регистрацией кассовых аппаратов и применением патентной системы налогообложения, а также проблемы с задним числом и коррекцией записей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725fdf-0689-4bc8-8373-a1b0943fadab",
   "metadata": {},
   "source": [
    "**Итак, кластеризация первых сообщений пользовтелей выполнена и топики получены.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09980727-4047-4a02-95c9-ab1c2050328f",
   "metadata": {},
   "source": [
    "**Чем я принебрег:**\n",
    "* Не учитывал те сессии, где запрос пользователя не был четко понятен в первом сообщении. Например, первой сообщение: *\"Добрый день\"*\n",
    "* Не учитывал взаимосвязь между сессиями\n",
    "\n",
    "__! Так же важно, что это был срез запросов пользователей и полученные результаты могут быть не репрезентативными__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55016e57-ce58-46f2-ad2f-e0ea6a5a7202",
   "metadata": {},
   "source": [
    "**Как можно попробовать улучшить:**\n",
    "* Поэксперементировать с моделью для эмбеддингов\n",
    "* Покрутить параметры для уменьшения размерности эмбеддингов и кластеризации, замеряя *Силуэтный коэффициент* (Я же просто руками перебрал некоторые параметры и глазами выбрал нормальное разбиение)\n",
    "* Использовать бóльший объем данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11584064-deda-4c44-81ba-9597cf7e7724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b1118df-fbfe-4217-bc11-bad5ad5b7366",
   "metadata": {},
   "source": [
    "Тут я проверял гипотезу про сессии:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60978e9-d348-49a6-b810-40cca91511fa",
   "metadata": {},
   "source": [
    "Смотрю по такой фразе и ищу где это первое сообщение в сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdb164-f335-4599-a15f-4d6c8b6749fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.str.contains(\"Спасибо, ответ помог\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36a797-4dbf-4d82-b45f-faf38988b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.session_id == \"!PRkcIndqEDmvYkBSWz-1.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15174bbe-c2f5-42b2-986e-481b28a7dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.session_id == \"!PRkcIndqEDmvYkBSWz-2.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4225f2-7292-466b-8ab8-ac51aa582eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.session_id == \"!PRkcIndqEDmvYkBSWz-3.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9abbb-b997-4dc5-ab22-d30777ec1bca",
   "metadata": {},
   "source": [
    "Видно, что сессии `!PRkcIndqEDmvYkBSWz-2.0` пользователь перестает отвечать, видимо, решает проблему, а через час в сессии `!PRkcIndqEDmvYkBSWz-3.0` благодарит за помощь.\n",
    "\n",
    "Из-за этого и получились странные результаты при кластеризации вначале"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48c93e-9552-44c0-8e62-061ba5c212b0",
   "metadata": {},
   "source": [
    "Также, кажется, что session_id сформирован так: f\"{хэш юзера}-{номер обращения}\"\n",
    "\n",
    "Таким образом, уменьшая номер обращения, можно получать контекст из предыдущих сессий. Но здесь данные урезаны, поэтому не всегда мы получим предыдущие сессии"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
